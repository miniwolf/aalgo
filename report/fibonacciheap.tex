\documentclass[a4paper,oneside,11pt]{article}
%%%%%%%%%%%%%%%%%%PACKAGES%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{graphicx}
%%%%%%%%%%%%%%%%%%COMMANDS%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Course}{Fibonacci Heap/ Binary Heap and Dijkstra Implementation}
%%%%%%%%%%%%%%%%%%PAGE STYLE%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\geometry{headheight=30pt,bmargin=3cm,tmargin=3.6cm}
\chead{\begin{large}\textbf{ \Course }\end{large}\\}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{E}}}

\begin{document}
\section*{Worst Case Analysis - Fibonacci Heap}
\subsection*{Make-heap}
It just creates an empty heap, and is implemented as our constructor.
\begin{itemize}
\item{No trees and no nodes}
\item{Worst case} $= O(1)$
\end{itemize}
\subsection*{Find-Min$(H)$}
By maintaining a pointer \texttt{minRoot}($min(H)$) which points at the node with minimum key.
\begin{itemize}
\item{Actual cost} = 1
\item{No changes in number of trees($t(H)$) or marked nodes($m(H)$)}
\item{Worst case} $= O(1)$
\end{itemize}
\subsection*{Insert$(H,x,k)$}
We add a tree with a single node to $H$ storing the item $x$ with our key $k$. We may also need to update our min($H$).
\begin{itemize}
\item{$t(H)$ is increased by 1, and we don't mark any nodes}
\item{Worst case}$ = 2 + 1 = O(1)$, 2 because we add a node and update min($H$)
\end{itemize}
\subsection*{Meld$(H_1, H_2)$}
We combine $H1$ and $H2$ together, forming a new heap $H$. We update $min(H)$ accordingly.
\begin{itemize}
\item{$t(H)$ and $m(H)$ is both unchanged}
\item{Worst case}$ = 2 + 0 = O(1)$, 2 because we put trees together and set $min(H)$
\end{itemize}
\subsection*{Delete-Min$(H)$}
We create the array of pointers which has a cost $\leq$ max degree of any node in our Fibonacci heap($d_n$) + 1.
Lets suppose we start with $k$ trees in our doubly linked list of roots, and perform $l$ link operations we then perform $k+l$ work, and end up with $k-l$ trees.
The worst case for linking the heap would be $n$ elements in our doubly-linked list of roots, with none of these combined in a tree.
\begin{itemize}
\item{$t(H)$ is reduces by $k/2$, $m(H)$ is not changed as we mark no nodes}

\subsection*{Find-Min$(H)$}
By maintaining a pointer \texttt{minRoot}($min(H)$) which points at the node with minimum key.
\begin{itemize}
\item{Actual cost} = 1
\item{No changes in number of trees($t(H)$) or marked nodes($m(H)$)}
\item{Worst case} $= O(1)$
\end{itemize}
\subsection*{Insert$(H,x,k)$}
We add a tree with a single node to $H$ storing the item $x$ with our key $k$. We may also need to update our min($H$).

\begin{itemize}
\item{$t(H)$ is increased by 1, and we don't mark any nodes}
\item{Worst case}$ = 2 + 1 = O(1)$, 2 because we add a node and update min($H$)
\end{itemize}
\subsection*{Meld$(H_1, H_2)$}
We combine $H1$ and $H2$ together, forming a new heap $H$. We update $min(H)$ accordingly.

\begin{itemize}
\item{$t(H)$ and $m(H)$ is both unchanged}
\item{Worst case}$ = 2 + 0 = O(1)$, 2 because we put trees together and set $min(H)$
\end{itemize}
\subsection*{Delete-Min$(H)$}

We create the array of pointers which has a cost $\leq$ max degree of any node in our Fibonacci heap($d_n$) + 1.

Lets suppose we start with $k$ trees in our doubly linked list of roots, and perform $l$ link operations we then perform $k+l$ work, and end up with $k-l$ trees.

The worst case for linking the heap would be $n$ elements in our doubly-linked list of roots, with none of these combined in a tree.
\begin{itemize}
\item{$t(H)$ is reduces by $k/2$, $m(H)$ is not changed as we mark no nodes}
\item{Worst case} $= O(n)$
\end{itemize}

potential change, $\Delta_i \geq -2l$

amortized cost, $\hat{c_i} = c_i + \Delta_i \leq 2(k - l) + d_n + 1$

But $k - l\leq d_n + 1$ (Because we can have at most one tree of each rank)

So, $\hat{c_i}\leq 3d_n + 3 = O(\log n)$.

\subsection*{Decrease Key$(H, x, k_x)$}
We decrease the key of the node $x$ to the new key $k_x$, and if the heap property becomes violated, the node is cut from its parent.
We also mark up the tree untill we hit a node or an unmarked node. For each cut we unmark a node when making the node a root.
\begin{itemize}
\item{$t(H)$ is increased by $k$, the number of cuts. $m(H)$ can at most be increased 1}
\item{$t(D_i)-t(D_{i-1}) = 1 + k$, $m(D_i)-m(D_{i-1})\leq -k+1$}
\item{Overall cost} = $c_i = 1 + k = O(k)$ each cut operation takes $O(1)$
\item{Worst case} = $O(\log n)$, where we have $\log n$ number of nodes we need to cut.
\end{itemize}

\begin{align*}
\text{potential drop, } \Delta_i &= 2(t(D_i)-t(D_{i-1}))+3(m(D_i)-m(D_{i-1}))\\
&\leq 2(1+k)+3(-k+1)\\
&= -k+5
\end{align*}

\begin{align*}
  \text{Amortized cost, }\hat{c_i} &= c_i + \delta_i\\
  &\leq (1+k)+(-k+5)\\
  &= 6\\
  &= O(1)
\end{align*}
\subsection*{Delete($H, x$)}
We delete by first decreasing the key of $x$ to the lowest value of our heap and then \texttt{DELETE-MIN} to remove it.

\begin{itemize}
\item{Worst case} $= O(\log n) + O(n) = O(n)$, worst case of \texttt{DECREASE-KEY} + worst case of \texttt{DELETE-MIN}
\end{itemize}

\begin{align*}
\text{amortized cost, } \hat{c_i} &= \text{ amortized cost of Decrease-Key}\\
&+\text{amortized cost of Delete-Min}\\
&= O(1) + O(\log n)\\
&= O(\log n)
\end{align*}

\section*{Worst Case Analysis - Binary Heap}
\subsection*{Make-heap}


\subsection*{Find-Min$(H)$}
Because of the heap property, the minimum element will always be present at the root of the heap.

\begin{itemize}
\item{Worst case} = $O(1)$.
\end{itemize}

\subsection*{Insert$(H,x,k)$}
The worst case complexity for inserts will be having to insert in the case of inserting the yet smallest element into the tree of height $h$ as this would make us bubble up the tree all the way into root this would give a running time of $O(h)$.

\begin{itemize}
  \item{Worst case} = $O(\log n)$.
\end{itemize}

\subsection*{Delete-Min$(H)$}
We start by deleting the minimum, we place the last element in the hole created at the root. This will leave the heap property possibly violated at the root level. So we bubble down until the violation is stopped.

\begin{itemize}
  \item{Worst case} = $O(\log n)$, the longest we have to bubble down to stop the violation.
\end{itemize}
\subsection*{Decrease Key$(H, x, k_x)$}
Lowers the current value of the item $x$, to new priority value $k_x$. Now the heap property could possibly be violated for the parent and node $x$. So we need to bubble $x$ up untill the violation is stopped.

\begin{itemize}
  \item{Worst case} = $O(\log n$, the longest we have to bubble up to stop the violation.
\end{itemize}

\subsection*{Delete($H, x$)}
First we use \texttt{DECREASE-KEY} to set the node $x$ to the lowest value, then we use \texttt{DELETE-MIN} to remove this node, because this is now the smallest.

\begin{itemize}
  \item{Worst case} = $O(\log n) + O(\log n) = O(\log n)$, the worst case for \texttt{DECREASE-KEY} + the worst case for \texttt{DELETE-MIN}.
\end{itemize}
\section*{Experiments - Fibonacci Heap}
\subsection*{Insert$(H,x,k)$}
\subsubsection*{Hypothesis}
We have that \texttt{FIBONACCI-HEAP-INSERT} performs in $O(1)$ time. This means that when we perform $N$ insertions into a fibonacci-heap we expect to measure a running time that increases linearly with the number of insertions, yielding a running time of $O(N)$ for $N$ insertions.

The experiment consists of inserting $N$ elements into the heap. The element's keys range from $2N$ to $N$, which has no effect on the execution time of \texttt{FIBONACCI\--HEAP\--INSERT}, but is usefull for later experimentation with \texttt{FIBONACCI\--HEAP\--DECREASE\--KEY}.

Below we present the measured running times for \texttt{FIBONACCI\--HEAP\--INSERT} for $N$ elements.
\subsubsection*{Graphs}
\begin{table}
  \begin{center}
    \begin{tabular}{l|r|r}
      $N$ & measured insert time(s) & measured insert/N time(s)\\
      \hline
      10       & \num{5.971E-7}     & \num{5.971E-8}\\
      100      & \num{3.424E-6}     & \num{3.424E-8}\\
      1000     & \num{4.215E-5}     & \num{4.215E-8}\\
      10000    & \num{5.587E-4}     & \num{5.588E-8}\\
      100000   & \num{0.0056}       & \num{5.668E-8}\\
      1000000  & \num{0.0610}       & \num{6.100E-8}\\
      10000000 & \num{0.6070}       & \num{6.070E-8}
    \end{tabular}
    \caption{Bounds and measured running time}
  \end{center}
\end{table}
\begin{figure}
  \includegraphics{time_per_insert_operation.png}
  \caption{Time per insert operation}
\end{figure}
\begin{figure}
  \includegraphics{insert_operations_and_theoretic_bound.png}
  \caption{Comparison - Insert operations and Theoretic Bound}
\end{figure}

\subsubsection*{Conclusion}
The time spent per single insert operation hovers around a constant factor. Even though the numbers do not form a horizontal line on the graph, they are in the same order of magnitude, excluding the possibility of an exponential growth. The second graph gives a good impression of \texttt{FIBONACCI-HEAP-INSERT} running in constant time for a single element, as the number of elements to be inserted tenfold, the running time also tenfolds. Thus we can safely assume that the \texttt{FIBONACCI-HEAP-INSERT} method actually runs in constant time.

\subsection*{Decrease-Key$(H,x,k)$}
\subsubsection*{Hypothesis}

The amortized running time for \texttt{FIBONACCI-HEAP-DECREASE-KEY} is $O(1)$ and the worst case running time is $O(\log n)$.

The experiment consists of a heap with $N$ elements with increasing keys from $N$ to $2N$, where \texttt{FIBONACCI-HEAP-DECREASE-KEY} is called with elements in decreasing order of their key, reducing their key by $N$, which means that the heap updates it's minimum element with each call.

The expectation is, that after some calls to \texttt{FIBONACCI-HEAP-DECREASE-KEY}, the cost of calling \texttt{FIBONACCI-HEAP-DECREASE-KEY} will approach a constant, which implies that $N$ calls to \texttt{FIBONACCI-HEAP-DECREASE-KEY} will have an amortited running time of $O(N)$.
\subsubsection*{Graphs}
\begin{table}
  \begin{center}
    \begin{tabular}{l|r|r}
      $N$ & measured decrease time(s) & measured decrease time(s) /N \\
      \hline
      10       & \num{5.492E-7}     & \num{5.492E-8}\\
      100      & \num{3.590E-6}     & \num{3.589E-8}\\
      1000     & \num{3.244E-5}     & \num{3.245E-8}\\
      10000    & \num{3.195E-4}     & \num{3.200E-8}\\
      100000   & \num{0.0034}       & \num{3.434E-8}\\
      1000000  & \num{0.0372}       & \num{3.718E-8}\\
      10000000 & \num{0.3847}       & \num{3.847E-8}
    \end{tabular}
    \caption{Bounds and measured running time}
  \end{center}
\end{table}
\begin{figure}
  \includegraphics{decrease_key_per_element.png}
  \caption{Decrease Key per element}
\end{figure}

\subsubsection*{Conclusion}
The measurement divided by the number of \texttt{FIBONACCI-HEAP-DECREASE-KEY} calls approach a factor of \num{3.786E-8}, within a small margin of error, when we ignore the first measurement of size 10, as this is easily inflated due to its very short running time.

We can safely conclude that the cost of each \texttt{FIBONACCI-HEAP-DECREASE-KEY} call converges to a constant factor, after a number of calls to the method, though one could still take measurements to find after how many calls in succession \texttt{FIBONACCI-HEAP-DECREASE-KEY} comes close to constant running time.

\subsection*{Delete-Min$(H)$}
\subsubsection*{Hypothesis}
The worst case running time for \texttt{FIBONACCI-HEAP-DELETE-MIN} is $O(n)$ and the amortized running time is $O(\log n)$. In this experiment, we start out with a heap of size $N$ and call \texttt{FIBONACCI-HEAP-DELETE-MIN} until the heap is empty. For this amount of delete-min operations, the total running time is expected to be $O(\log(N!))$.

Due to the amortized running time, we expect $N$ \texttt{FIBONACCI-HEAP-DELETE-MIN} operations to perform much better than $G(N) =$ \[\sum^N_{n=1} n.\] $ = N*(N-1)/2$.

\subsubsection*{Graphs}
\begin{table}
  \begin{center}
    \begin{tabular}{l|r|r|r}
      $N$ & expected running time(s) & measured running time(s) & $G(N)$\\
      \hline
      10       & \num{20}           & \num{1.981E-6} & \num{45}\\ 
      100      & \num{522}          & \num{2.523E-5} & \num{4950}\\
      1000     & \num{8504}         & \num{0.000254} & \num{499500}\\
      10000    & \num{118261}       & \num{0.00383}  & \num{49995000}\\
      100000   & \num{1516460}      & \num{0.04043}  & \num{5E+9}\\
      1000000  & \num{18488855}     & \num{0.43195}  & \num{5E+11}\\
      10000000 & \num{218107833}    & \num{3.71978}  & \num{5E+13}
    \end{tabular}
    \caption{Bounds and measured running time}
  \end{center}
\end{table}
\begin{figure}
  \includegraphics{ghj}
  \caption{ghj}
\end{figure}

\subsubsection*{Conclusion}
The measured running times show that the running time increases at most at the same rate as the theoretical bound we expected and increases not at all like the worst-case bound for $N$ insertions. We can conclude that this implementation of \texttt{FIBONACCI-HEAP-DELETE-MIN} operates in the expected amortized bound of $O(\log n)$.

\section*{Experiments - Binary Heap}
\subsection*{Insert$(H,x,k)$}
In this section will we explore the running time of Binary-Heaps insert method. The running time is analyzed in two different ways. At first, we run a number of inserts accumulative and observe the total running time of $N$ \texttt{BINARY-HEAP-INSERT} operations. Then we analyze \texttt{BINARY-HEAP-INSERT} operations relative to the height of the heap.
\subsubsection*{Hypothesis}
To measure the running time, $N=10^k$ elements with decreasing key are inserted into the heap in succession to achieve worst-case behaviour. This is done 50 times to compute an average.

Because the \texttt{BINARY-HEAP-INSERT} operation is done $N$ times in succession, we expect the running time to be the sum over the individual insert operation at any given size of the heap, which is called $n$.
Thus, we expect to measure a running time of the form: \[\sum^N_{n=1}\lg n.\]
We have that:
\begin{align*}
  \lg(a)+\lg(b)&=\lg(a*b)\text{, then}\\
  f(N)=\sum^N_{n=1}\lg(n)&=\lg(1)+\lg(2)+\cdots+\lg(N)\\
  &=\lg(1*2*3*\cdots *N)\\
  &=\lg(N!)
\end{align*}

The numbers for f(N) in the graph section are only approximate, as computing the faculty of numbers bigger than 100 is troublesome.

In the second experiment, an emtpy heap is used, and elements are inserted layer by layer. To increase the height of the heap by one every time, we insert $2^k$ elements, where k is the height of the heap. Each element is inserted with a key that is smaller than the smallest key currently in the heap, which means that the element has to travel through the $k$ layers in the heap to reach the top. This is the worst case scenario for \texttt{BINARY-HEAP-INSERT}.

For each layer, we measure the time for running \texttt{BINARY-HEAP-INSERT} $2^k$ times and divide the running time by the numbers of elements inserted, which yields an average running time for the current height of the graph.

We also count the number of comparisons made for each size, which we expect to be the logarithm of the height of the heap and time single inserts at the same height 50 times to compute an average time per insertion at the given size.
\subsubsection*{Graphs}

In the table below, we present the measured running times, and the computed function $f(N)$.
\begin{table}
  \begin{center}
    \begin{tabular}{l|r|r}
      $N$ & $f(N)$ & measured running time(s) \\
      \hline
      10       & 20         & \num{1.6487E-6}\\
      100      & 522        & \num{2.3691E-5}\\
      1000     & 8504       & \num{2.6033E-4}\\
      10000    & 118261     & \num{3.4728E-3}\\
      100000   & 1516460    & \num{4.1341E-2}\\
      1000000  & 18488855   & \num{0.4934}\\
      10000000 & 218107833  & \num{5.5171}
    \end{tabular}
    \caption{Bounds and measured running time}
  \end{center}
\end{table}
\begin{table}
  \begin{center}
    \begin{tabular}{l|r|r}
      size & number of comparisons & time per comparison\\
      \hline
      10       & 3  & \num{4.556E-7}\\
      100      & 6  & \num{4.507E-7}\\
      1000     & 9  & \num{4.514E-6}\\
      10000    & 13 & \num{6.597E-7}\\
      100000   & 16 & \num{7.105E-7}\\
      1000000  & 19 & \num{1.907E-5}\\
      10000000 & 23 & \num{1.081E-6}
    \end{tabular}
    \caption{Bounds and measured running time}
  \end{center}
\end{table}
\begin{figure}
  \includegraphics{binary_insert_runningtime.png}
  \caption{Running time compared to theoretical bound}
\end{figure}

\subsubsection*{Conclusion}
If we plot the graph for the proposed function and the measured running time on a doubly logarithmic graph, then we see they have the same tendency. We can safely discard the measured running times for 10 elements, as running times this close to single microseconds may easiliy be inflated. We observe that the running time increase at the same rate as the theoretical bound, this means that our measured running time conforms to the expected running time $O(\log N!)$. Hereby the running time of binary heap's insert conforms to the expected asymptotic running time $O(\log n)$.

As expected, the number of comparisons for an insert operation is logarithmic in the size of the heap. Comparing the running times for insertion divided by the number of comparisons shows some irregularities. At heap size of 10, 100, 10.000, 100.000 and 10.000.000 are close to the same constant, the values for heap sizes of 1.000 and 1.000.000 vary wildly. This can possibly be explained by the elements being compared sometimes being on few pages of memory, whilst other times being spread amongst multiple pages of memory, though we have no further data to back up this claim.
\subsection*{Decrease Key$(H,x,k)$}
\subsubsection*{Hypothesis}
The \texttt{BINARY\--HEAP\--DECREASE\--KEY} method has a worst case running time of $O(\log n)$ where $n$ is the current size of the heap.
This means, when we run \texttt{BINARY\--HEAP\--DECREASE\--KEY} on every element, the asymptotic running is expected to be $O(n\log n)$ in the worst case.

Our experiment consists of a Binary Heap where $N$ elements were inserted with decreasing keys from $2N$ to $N+1$ and we measure the time it takes to call \texttt{BINARY\--HEAP\--DECREASE\--KEY} on every element once. We start with the element with the biggest key, $2N$, and decrease its key to $N$. This gives us the worst case for the \texttt{BINARY\--HEAP\--DECREASE\--KEY} methods, as $N$ is the smallest key at this point. We repeat this process for the next biggest element, which has key $2N-1$ and decrease its key to $N-1$, again giving rise to the worst case scenario. We repeat this process for all remaining elements, decreasing the key of the element with key $2N-i$ to $N-i$.

As we repeat this process for $N$ elements, the total running time for $N$ \texttt{BINARY\--HEAP\--DECREASE\--KEY} operations is $O(N\log N)$.

In the table below, we present the measured running times, and the function $O(N\log N)$.
\subsubsection*{Graphs}
\begin{table}
  \begin{center}
    \begin{tabular}{l|r|r}
      $N$ & $O(N\log N)$ & measured running time(s)\\
      \hline
      10       & 33         & \num{7.506E-7}\\
      100      & 644        & \num{1.0843E-5}\\
      1000     & 9966       & \num{0.000141}\\
      10000    & 132877     & \num{0.001805}\\
      100000   & 1660964    & \num{0.0242}\\
      1000000  & 19931569   & \num{0.3163}\\
      10000000 & 232534967  & \num{3.7488}
    \end{tabular}
    \caption{Bounds and measured running time}
  \end{center}
\end{table}
\begin{figure}
    \includegraphics{binary_decreasetime.png}
    \caption{Decrease-Key running time compared to theoretical bound.}
\end{figure}

\subsubsection*{Conclusion}
Observe that the graph shows the same inclination for the measured data and the $n\log n$ function. The measured running time increases proportionally just as much as the $n\log n$ theoretical bound, thereby a single \texttt{BINARY-HEAP-DECREASE-KEY} performs in $O(\log n)$ time.

\subsection*{Delete Min$(H)$}
\subsubsection*{Hypothesis}
The \texttt{BINARY-HEAP-DELETE-MIN} method has a worst case running time of $O(\log n)$ where $n$ is the current size of the heap. If we call \texttt{BINARY-HEAP-DELETE-MIN} as many times as there are elements in the heap, we would have a running of $O(\log N!)$, exactly like \texttt{BINARY-HEAP-INSERT}.

The experiment consists of calling \texttt{BINARY-HEAP-DELETE-MIN} until the heap of size $N$ is empty.

In the table below, we present the measured running times, and the function $O(N\log N)$. The second table shows the running times for insertion divided by the running times for deletion.
\subsubsection*{Graphs}
\begin{table}
  \begin{center}
    \begin{tabular}{l|r|r}
      $N$ & $O(N\log N)$ & measured running time(s)\\
      \hline
      10       & 33         & \num{1.8486E-6}\\
      100      & 644        & \num{2.3476E-5}\\
      1000     & 9966       & \num{2.8870E-4}\\
      10000    & 132877     & \num{3.4556E-3}\\
      100000   & 1660964    & \num{0.0410}\\
      1000000  & 19931569   & \num{0.4842}\\
      10000000 & 232534967  & \num{5.7055}
    \end{tabular}
    \caption{Bounds and measured running time}
  \end{center}
\end{table}

\begin{table}
  \begin{center}
    \begin{tabular}{l|r}
      $N$ & $\dfrac{\text{insert}}{\text{delete}}$\\
      \hline
      10       & \num{0.89}\\
      100      & \num{1.01}\\
      1000     & \num{0.90}\\
      10000    & \num{1.00}\\
      100000   & \num{1.01}\\
      1000000  & \num{1.02}\\
      10000000 & \num{0.97}
    \end{tabular}
    \caption{Bounds and measured running time}
  \end{center}
\end{table}
\begin{figure}
    \includegraphics{binary_delete_min.png}
    \caption{Decrease-Key running time compared to theoretical bound.}
\end{figure}

\subsubsection*{Conclusion}
Observe that the measurements for \texttt{BINARY-HEAP-DELETE-MIN} increase at most as fast as the theoretical bound, but also are very close to the measured running times of \texttt{BINARY-HEAP-INSERT}. When we divide the running time of \texttt{BINARY-HEAP-INSERT} with the runnning time of \texttt{BINARY-HEAP-DELETE-MIN} for every given size, we find that the average factor is 0.97 within a small margin of error. We can conclude that the running time for deletion increases at the same rate as those for insertion, which gives us that our implementation of \texttt{BINARY-HEAP-DELETE-MIN} performs within $O(\log n)$ bounds, just like insertion.

\section*{Experiments - Comparison}
Comparing the operations for Binary Heap and Fibonacci Heap shows that insertions are always faster in Fibonacci Heaps, as these can be performed in constant time opposed to the logarithmic worst time performance for binary heaps. Also when purely calling \texttt{DECREASE\--KEY} many times in a row, a Fibonacci Heap performs better and better compared to a Binary Heap, the bigger the heap is. Only the \texttt{DELETE\--MIN} methods perform comparable with about a factor of one, until heap sizes of 10.000.000 are used, where Fibonacci Heaps starts to perform slightly faster.

\section*{Dijkstra}
\end{document}
