\documentclass[a4paper,oneside,11pt]{article}
%%%%%%%%%%%%%%%%%%PACKAGES%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%COMMANDS%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\Course}{Fibonacci Heap/ Binary Heap and Dijkstra Implementation}

%%%%%%%%%%%%%%%%%%PAGE STYLE%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\geometry{headheight=30pt,bmargin=3cm,tmargin=3.6cm}
\chead{\begin{large}\textbf{ \Course }\end{large}\\}

\sisetup{output-exponent-marker=\ensuremath{\mathrm{E}}}

\begin{document}
\section*{Worst Case Analysis - Fibonacci Heap}
\subsection*{Make-heap}
It just creates an empty heap, and is implemented as our constructor.
\begin{itemize}
\item{No trees and no nodes}
\item{Worst case} $= O(1)$
\end{itemize}
\subsection*{Find-Min$(H)$}
By maintaining a pointer \texttt{minRoot}($min(H)$) which points at the node with minimum key.
\begin{itemize}
\item{Actual cost} = 1
\item{No changes in number of trees($t(H)$) or marked nodes($m(H)$)}
\item{Worst case} $= O(1)$
\end{itemize}
\subsection*{Insert$(H,x,k)$}
We add a tree with a single node to $H$ storing the item $x$ with our key $k$. We may also need to update our min($H$).

\begin{itemize}
\item{$t(H)$ is increased by 1, and we don't mark any nodes}
\item{Worst case}$ = 2 + 1 = O(1)$, 2 because we add a node and update min($H$)
\end{itemize}
\subsection*{Meld$(H_1, H_2)$}
We combine $H1$ and $H2$ together, forming a new heap $H$. We update $min(H)$ accordingly.

\begin{itemize}
\item{$t(H)$ and $m(H)$ is both unchanged}
\item{Worst case}$ = 2 + 0 = O(1)$, 2 because we put trees together and set $min(H)$
\end{itemize}
\subsection*{Delete-Min$(H)$}
We create the array of pointers which has a cost $\leq$ max degree of any node in our Fibonacci heap($d_n$) + 1.

Lets suppose we start with $k$ trees in our doubly linked list of roots, and perform $l$ link operations we then perform $k+l$ work, and end up with $k-l$ trees.

The worst case for searching for the new \texttt{minRoot} would be $n$ elements in our doubly-linked list of roots, with none of these combined in a tree.
\begin{itemize}
\item{$t(H)$ is reduces by $k/2$, m($H$) is not changed as we mark no nodes}
\item{Worst case} $= O(n)$
\end{itemize}

potential change, $\Delta_i \geq -2l$

amortized cost, $\hat{c_i} = c_i + \Delta_i \leq 2(k - l) + d_n + 1$

But $k - l\leq d_n + 1$ (Because we can have at most one tree of each rank)

So, $\hat{c_i}\leq 3d_n + 3 = O(\log n)$.

\subsection*{Decrease Key$(H, x, k_x)$}
We decrease the key of the node $n$ to the new key $k$, and if the heap property becomes violated, the node is cut from its parent.
We also mark up the tree untill we hit a node or an unmarked node. For each cut we unmark a node when making the node a root.
\begin{itemize}
\item{$t(H)$ is increased by $k$, the number of cuts. $m(H)$ can at most be increased 1}
\item{$t(D_i)-t(D_{i-1}) = 1 + k$, $m(D_i)-m(D_{i-1})\leq -k+1$}
\item{Overall cost} = $c_i = 1 + k = O(k)$ each cut operation takes $O(1)$
\item{Worst case} = $O(n)$, where we have $n$ number of nodes we need to make a new root.
\end{itemize}

\begin{align*}
\text{potential drop, } \Delta_i &= 2(t(D_i)-t(D_{i-1}))+3(m(D_i)-m(D_{i-1}))\\
&\leq 2(1+k)+3(-k+1)\\
&= -k+5
\end{align*}

\begin{align*}
  \text{Amortized cost, }\hat{c_i} &= c_i + \delta_i\\
  &\leq (1+k)+(-k+5)\\
  &= 6\\
  &= O(1)
\end{align*}
\subsection*{Delete($H, x$)}
We delete by first decreasing the key of $x$ to the lowest value of our heap and then \texttt{Delete-Min} to remove it.

\begin{itemize}
\item{Worst case} $= O(n) + O(n) = O(n)$, worst case of \texttt{Decrease-Key} + worst case of \texttt{Delete-Min}
\end{itemize}

\begin{align*}
\text{amortized cost, } \hat{c_i} &= \text{ amortized cost of Decrease-Key}\\
&+\text{amortized cost of Delete-Min}\\
&= O(1) + O(\log n)\\
&= O(\log n)
\end{align*}

\section*{Worst Case Analysis - Binary Heap}
\subsection*{Make-heap}

\subsection*{Find-Min$(H)$}

\subsection*{Insert$(H,x,k)$}

\subsection*{Meld$(H_1, H_2)$}

\subsection*{Decrease Key$(H, x, k_x)$}

\subsection*{Delete($H, x$)}

\section*{Experiments - Fibonacci Heap}
\subsection*{Insert$(H,x,k)$}
We expect constant running time 

\section*{Experiments - Binary Heap}
\subsection*{Insert$(H,x,k)$}
In this section will we explore the running time of Binary-Heaps insert method.
\subsubsection*{Hypothesis}
To measure the running, $N=10^k$ randomly generated elements are inserted into the heap in succession. This is done 50 times to compute an average time. The elements inserted into the heap are randomly generated before we start timing the \texttt{BINARY-HEAP-INSERT} method $N$ times.

Because the \texttt{BINARY-HEAP-INSERT} operation is done $N$ times in succession, we expect the running time to be the sum over the individual insert operation at any given size of the heap, which is called $n$.
Thus, we expect to measure a running time of the form: \[\sum^N_{n=1}\lg n.\]
We have that:
\begin{align*}
  \lg(a)+\lg(b)&=\lg(a*b)\text{, then}\\
  f(N)=\sum^N_{n=1}\lg(n)&=\lg(1)+\lg(2)+\cdots+\lg(N)\\
  &=\lg(1*2*3*\cdots *N)\\
  &=\lg(N!)
\end{align*}
\subsubsection*{Graphs}

In the table below, we present the measured running times, and the computed function $f(N)$.
\begin{table}
  \begin{center}
    \begin{tabular}{l|r|r}
      $N$ & $f(N)$ & measured running time(s)\\
      \hline
      10       & 6         & \num{5.60E-6}\\
      100      & 157       & \num{2.34E-5}\\
      1000     & 2560      & \num{1.93E-4}\\
      10000    & 35600     & \num{2.25E-3}\\
      100000   & 456500    & \num{2.54E-2}\\
      1000000  & 5565700   & \num{0.279}\\
      10000000 & 65657000  & \num{2.770}
    \end{tabular}
    \caption{Expected and measured running time}
  \end{center}
\end{table}
\begin{figure}
  \includegraphics{binary_insert_runningtime.png}
  \caption{Running time compared to expected time}
\end{figure}

\subsubsection*{Conclusion}
If we plot the graph for the proposed and measured running time on a doubly logarithmic graph, then we see they have the same tendency. We can safely discard the measured running times for 10 elements, as running times this close to single microseconds may easiliy be inflated when not using a high-performance timing API.
This means that our measured running time conforms to the expected running time $O(\log N!)$. Hereby the running time of binary heap's insert conforms to the expected asymptotic running time $O(\log n)$.

\subsection*{Decrease Key$(H,x,k)$}

\subsubsection*{Hypothesis}
The \texttt{BINARY-HEAP-DECREASE-KEY} method has a worst case running time of $O(\log n)$ where n is the current size of the heap. This means, when we run \texttt{BINARY-HEAP-DECREASE-KEY} on every element, the asymptotic running is expected to be $O(n\log n)$ 

Our experiment consists of a Binary Heap where $N$ random elements were inserted and we measure the time it takes to call \texttt{BINARY-HEAP-DECREASE-KEY} on every element once. The elements keys are halved. This is done 50 times and we present the average running time.
\subsubsection*{Graphs}

\subsubsection*{Conclusion}
Observe that the graph shows the same inclination for the measured data and the $n\log n$ function, when we discard the measurements for 10 elements. The measured running time increases proportionally just as much as the $n\log n$ function, thereby a single \texttt{BINARY-HEAP-DECREASE-KEY} performs in $O(\log n)$ time.

\subsection*{Delete Min$(H,x,k)$}

\subsubsection*{Hypothesis}
The \texttt{BINARY-HEAP-DELETE-MIN} method has a worst case running time of $O(\log n)$ where n is the current size of the heap.
\subsubsection*{Graphs}

\subsubsection*{Conclusion}

\section*{Experiments - Comparison}

\end{document}
